## paper list
* [SpanBERT: Improving Pre-training by Representing and Predicting Spans] [[paper](https://arxiv.org/pdf/1907.10529.pdf "Mandar Joshi, Danqi Chen, Yinhan Liu, Daniel S. Weld, Luke Zettlemoyer, Omer Levy")] (2020) `SpanBERT`
* [RoBERTa: A Robustly Optimized BERT Pretraining Approach] [[paper](https://arxiv.org/pdf/1907.11692.pdf "Yinhan Liu, Myle Ott, Naman Goyal, Jingfei Du, Mandar Joshi, Danqi Chen, Omer Levy, Mike Lewis, Luke Zettlemoyer, Veselin Stoyanov")] (2019) `RoBERTa`
* [XLNet: Generalized Autoregressive Pretraining for Language Understanding] [[paper](https://papers.nips.cc/paper/8812-xlnet-generalized-autoregressive-pretraining-for-language-understanding.pdf "Zhilin Yang, Zihang Dai, Yiming Yang, Jaime Carbonell, Ruslan Salakhutdinov, Quoc V. Le")] (2019) `XLNet`
* [Pre-Training with Whole Word Masking for Chinese BERT] [[paper](https://arxiv.xilesou.top/pdf/1906.08101.pdf "Yiming Cui, Wanxiang Che, Ting Liu, Bing Qin, Ziqing Yang, Shijin Wang, Guoping Hu")] (2019) `BERT-WWM(Chinese)`
* [ERNIE: Enhanced Language Representation with Informative Entities] [[paper](https://arxiv.xilesou.top/pdf/1905.07129.pdf "Zhengyan Zhang, Xu Han, Zhiyuan Liu, Xin Jiang, Maosong Sun, Qun Liu")] (2019) `ERNIE(知识图谱)`
* [ERNIE 2.0: A Continual Pre-Training Framework for Language Understanding] [[paper](https://arxiv.xilesou.top/pdf/1907.12412.pdf?source=post_page "Yu Sun, Shuohuan Wang, Yukun Li, Shikun Feng, Hao Tian, Hua Wu, Haifeng Wang")] (2019) `ERNIE(百度)`
* [ERNIE: Enhanced Representation through Knowledge Integration] [[paper](https://arxiv.xilesou.top/pdf/1904.09223.pdf "Yu Sun, Shuohuan Wang, Yukun Li, Shikun Feng, Xuyi Chen, Han Zhang, Xin Tian, Danxiang Zhu, Hao Tian, Hua Wu")] (2019) `ERNIE(百度)`


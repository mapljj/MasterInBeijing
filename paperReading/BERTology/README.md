# overview
BERT(Bidirectional Encoder Representation from Transformers), came from Google, is a powerful pretrain model for NLP's tasks. Here are some related papers before or about BERT. And progress of **pretrain** and **compression/acceleration** after BERT.
## before BERT
* [BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding] [[paper](https://arxiv.org/pdf/1810.04805.pdf "Jacob Devlin, Ming-Wei Chang, Kenton Lee, Kristina Toutanova")] (code  [[google](https://github.com/google-research/bert)][[huggingface](https://github.com/huggingface/transformers)])(2018) `BERT`
* [Improving Language Understanding by Generative Pre-Training] [[paper](https://www.cs.ubc.ca/~amuham01/LING530/papers/radford2018improving.pdf "Alec Radford, Karthik Narasimhan, Tim Salimans, Ilya Sutskever")] (2018) `GPT-1`
* [Attention Is All You Need] [[paper](https://papers.nips.cc/paper/7181-attention-is-all-you-need.pdf "Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N. Gomez, ≈Åukasz Kaiser, Illia Polosukhin")] (2017) `Transformer`
* [GloVe: Global Vectors for Word Representation] [[paper](https://www.aclweb.org/anthology/D14-1162.pdf "Jeffrey Pennington, Richard Socher, Christopher D. Manning")] (2014) `GloVe`
* [Efficient Estimation of Word Representations in Vector Space] [[paper](https://arxiv.xilesou.top/pdf/1301.3781.pdf "Tomas Mikolov, Kai Chen, Greg Corrado, Jeffrey Dean")] (2013) `word2vec(CBOW, skip-gram)`
* [Distributed Representations of Words and Phrases and their Compositionality] [[paper](https://papers.nips.cc/paper/5021-distributed-representations-of-words-and-phrases-and-their-compositionality.pdf "Tomas Mikolov, Ilya Sutskever, Kai Chen, Greg Corrado, Jeffrey Dean")] (2013) `word2vec(tricks)`
